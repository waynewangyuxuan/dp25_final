# Deep Learning Project: Jailbreaking Deep Models

This project demonstrates adversarial attacks on production-grade image classifiers, focusing on crafting minimal perturbations that significantly degrade model performance while remaining imperceptible.

## Quick Start

```bash
# Setup environment
chmod +x setup.sh && ./setup.sh

# Activate environment
source act.sh

# Run individual tasks
python experiments/task1_baseline.py  # Baseline evaluation
python experiments/task2_fgsm.py      # FGSM attack
python experiments/task3_advanced_attack.py  # PGD attack
python experiments/task4_pgd_patch.py  # Patch attack
python experiments/task5_transfer.py  # Transferability analysis

# Or use make
make all  # Run all tasks
make clean  # Clean generated files
```

## Important Directories

- ğŸ“ `src/` - Core implementation code and attack algorithms
- ğŸ“ `experiments/` - Scripts for running different attack experiments
- ğŸ“ `data/` - Dataset storage (git-ignored)
- ğŸ“ `logs/` - Experiment logs and metrics (git-ignored)
- ğŸ“ `logging/` - Logging configuration and utilities
- ğŸ“ `figures/` - Generated visualizations and plots (git-ignored)
- ğŸ“ `checkpoints/` - Model checkpoints and saved states (git-ignored)

## Project Structure

```
deepâ€‘modelsâ€‘jailbreak/
â”œâ”€â”€ src/                       # Core implementation code
â”‚   â”œâ”€â”€ dataset.py             # Dataset loading utilities
â”‚   â”œâ”€â”€ metrics.py             # Evaluation metrics
â”‚   â”œâ”€â”€ viz.py                 # Visualization utilities
â”‚   â””â”€â”€ attacks/               # Attack implementations
â”‚       â”œâ”€â”€ fgsm.py            # Single-step attack
â”‚       â”œâ”€â”€ pgd_full.py        # Iterative full-image attack
â”‚       â”œâ”€â”€ pgd_patch.py       # Iterative 32Ã—32 patch attack
â”‚       â””â”€â”€ utils.py           # Attack utilities
â”œâ”€â”€ experiments/               # Experiment scripts for each task
â”œâ”€â”€ configs/                   # Configuration files
â”œâ”€â”€ data/                      # Data directory (git-ignored)
â””â”€â”€ figures/                   # Generated visualizations
```

## Results Summary

### Task 1: Baseline Evaluation

ResNet-34 pretrained on ImageNet-1K evaluated on a 100-class subset:
- **Top-1 Accuracy**: 76.00%
- **Top-5 Accuracy**: 94.20%

### Task 2: FGSM Attack (Îµ=0.02)

Fast Gradient Sign Method generates adversarial examples with imperceptible perturbations:

![FGSM Examples](figures/task2/successful_attacks.png)

**Results**:
- **Top-1 Accuracy Drop**: 76.00% â†’ 6.80% (-69.20%)
- **Top-5 Accuracy Drop**: 94.20% â†’ 20.60% (-73.60%)
- **Max Lâˆ Distance**: 0.02

### Task 3: PGD Attack (Îµ=0.02)

Projected Gradient Descent with multiple iterations achieves complete model failure:

![PGD Examples](figures/task3/successful_attacks.png)

**Results**:
- **Top-1 Accuracy**: 0.00% (100% attack success)
- **Top-5 Accuracy**: 0.00%
- **Attack Parameters**: 
  - Îµ = 0.02 (perturbation budget)
  - Î± = 0.004 (step size)
  - iterations = 20
  - random_start = True

### Task 4: Patch Attack (32Ã—32 patch, Îµ=0.3)

Localized attack concentrates perturbations in a small 32Ã—32 patch:

![Patch Examples](figures/task4/patch_attack_examples.png)

**Results**:
- **Top-1 Accuracy Drop**: 76.00% â†’ 40.20% (-35.80%)
- **Top-5 Accuracy Drop**: 94.20% â†’ 65.80% (-28.40%)
- **Attack Success Rate**: 48.40%
- **Perturbation Budget**: Îµ = 0.3 (higher than full-image attacks)
- **Affected Area**: Only 4% of image pixels (32Ã—32 patch in 224Ã—224 image)

### Task 5: Transferability Analysis

Testing adversarial examples on DenseNet-121:

![Transfer Results](figures/task5/transfer_results.png)

**Results**:
| Attack Method | Transfer Rate | Effectiveness |
|---------------|--------------|--------------|
| FGSM (Îµ=0.02) | 1.1% | Limited |
| PGD (Îµ=0.02) | 24.9% | Limited |
| Patch (Îµ=0.3) | 54.4% | Moderate |

**Key Insight**: Patch attacks show surprisingly high transferability despite affecting fewer pixels.

## Conclusion

This project demonstrates:

1. State-of-the-art image classifiers remain highly vulnerable to imperceptible adversarial perturbations
2. PGD can achieve 100% attack success rate while respecting strict perturbation constraints
3. Even constraining perturbations to small image patches yields significant accuracy drops
4. Patch-based attacks transfer better across model architectures than full-image attacks
5. Implementation details (normalization handling, initialization, gradient computation) significantly impact attack effectiveness

These findings raise important questions about the robustness and security of deep learning systems in real-world applications.